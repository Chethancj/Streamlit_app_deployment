# -*- coding: utf-8 -*-
"""streamlit code 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M-3GVh8sxDfpebbzP39YrZCGKHMfxOY2
"""

import numpy as np
import pandas as pd

# For reproducibility
np.random.seed(42)

# Number of base samples
n_samples = 40000


# physiological features

glucose = np.random.normal(loc=140, scale=30, size=n_samples)  # mg/dL
carbs = np.random.normal(loc=50, scale=15, size=n_samples)  # grams
insulin = np.random.normal(loc=8, scale=3, size=n_samples)  # units
activity = np.random.normal(loc=30, scale=10, size=n_samples)  # minutes/day
stress = np.random.choice([0, 1], size=n_samples, p=[0.7, 0.3])  # 0 = low, 1 = high
time_of_day = np.random.choice([0, 1, 2], size=n_samples, p=[0.3, 0.4, 0.3])  # 0=morning, 1=afternoon, 2=evening


#  clinical features

age = np.random.normal(loc=55, scale=12, size=n_samples).clip(18, 90)
bmi = np.random.normal(loc=28, scale=5, size=n_samples).clip(15, 50)
hba1c = np.random.normal(loc=7.5, scale=1.2, size=n_samples).clip(4, 15)
hypertension = np.random.choice([0, 1], size=n_samples, p=[0.65, 0.35])
kidney_disease = np.random.choice([0, 1], size=n_samples, p=[0.85, 0.15])
type_diabetes = np.random.choice([1, 2], size=n_samples, p=[0.3, 0.7])
duration_since_dx = np.random.normal(loc=8, scale=6, size=n_samples).clip(0, 50)  # years


# Lifestyle features

sleep_hours = np.random.normal(loc=7, scale=1.5, size=n_samples).clip(3, 12)
diet_quality = np.random.choice([1, 2, 3], size=n_samples, p=[0.3, 0.5, 0.2])  # 1=poor, 3=good
smoking_status = np.random.choice([0, 1, 2], size=n_samples, p=[0.6, 0.25, 0.15])  # 0=non,1=former,2=current
alcohol_units_week = np.random.poisson(lam=3, size=n_samples)  # units/week
med_adherence = np.random.choice([0, 1, 2], size=n_samples, p=[0.7, 0.2, 0.1])  # 0=adherent,1=missed,2=irregular


# Time-trend glucose features

glucose_trend_3h = np.random.choice([-1, 0, 1], size=n_samples, p=[0.25, 0.5, 0.25])  # -1=falling,0=stable,1=rising
avg_glucose_7d = np.random.normal(loc=145, scale=25, size=n_samples)
time_since_last_insulin = np.random.normal(loc=4, scale=2, size=n_samples).clip(0, 24)  # hours
hyper_events_past_week = np.random.poisson(lam=2, size=n_samples)


#  probabilities for multi-class hyperglycaemia

# We'll create a continuous risk score and then map to severity categories
beta = {
    "intercept": -5,
    "glucose": 0.035,
    "carbs": 0.015,
    "insulin": -0.04,
    "activity": -0.02,
    "stress": 0.7,
    "time_afternoon": 0.25,
    "time_evening": 0.45,
    "age": 0.008,
    "bmi": 0.015,
    "hba1c": 0.35,
    "hypertension": 0.4,
    "kidney_disease": 0.6,
    "type_diabetes": 0.3,
    "duration_since_dx": 0.01,
    "sleep_hours": -0.05,
    "diet_quality": -0.1,
    "smoking_status": 0.15,
    "alcohol_units_week": 0.02,
    "med_adherence": 0.4,
    "glucose_trend_3h": 0.5,
    "avg_glucose_7d": 0.02,
    "time_since_last_insulin": 0.03,
    "hyper_events_past_week": 0.25
}

time_afternoon = (time_of_day == 1).astype(int)
time_evening = (time_of_day == 2).astype(int)

logit = (
    beta["intercept"]
    + beta["glucose"] * glucose
    + beta["carbs"] * carbs
    + beta["insulin"] * insulin
    + beta["activity"] * activity
    + beta["stress"] * stress
    + beta["time_afternoon"] * time_afternoon
    + beta["time_evening"] * time_evening
    + beta["age"] * age
    + beta["bmi"] * bmi
    + beta["hba1c"] * hba1c
    + beta["hypertension"] * hypertension
    + beta["kidney_disease"] * kidney_disease
    + beta["type_diabetes"] * (type_diabetes - 1)  # type 2 gets 0, type 1 gets coeff
    + beta["duration_since_dx"] * duration_since_dx
    + beta["sleep_hours"] * sleep_hours
    + beta["diet_quality"] * diet_quality
    + beta["smoking_status"] * smoking_status
    + beta["alcohol_units_week"] * alcohol_units_week
    + beta["med_adherence"] * med_adherence
    + beta["glucose_trend_3h"] * glucose_trend_3h
    + beta["avg_glucose_7d"] * avg_glucose_7d
    + beta["time_since_last_insulin"] * time_since_last_insulin
    + beta["hyper_events_past_week"] * hyper_events_past_week
)

prob = 1 / (1 + np.exp(-logit))


# severity categories
severity_labels = ['Hypo', 'Normal', 'Mild Hyperglycemia', 'Moderate Hyperglycemia', 'Severe Hyperglycemia']
severity_classes = [0, 1, 2, 3]
severity = np.select(
    [
        glucose < 70,
        (glucose >= 70) & (glucose < 180),
        (glucose >= 180) & (glucose < 250),
        (glucose >= 250) & (glucose < 300),
        glucose >= 300
    ],
    [0, 1, 2, 3, 4]
)


# Build DataFrame

df = pd.DataFrame({
    "glucose": glucose,
    "carbs": carbs,
    "insulin": insulin,
    "activity": activity,
    "stress": stress,
    "time_of_day": time_of_day,
    "age": age,
    "bmi": bmi,
    "hba1c": hba1c,
    "hypertension": hypertension,
    "kidney_disease": kidney_disease,
    "type_diabetes": type_diabetes,
    "duration_since_dx": duration_since_dx,
    "sleep_hours": sleep_hours,
    "diet_quality": diet_quality,
    "smoking_status": smoking_status,
    "alcohol_units_week": alcohol_units_week,
    "med_adherence": med_adherence,
    "glucose_trend_3h": glucose_trend_3h,
    "avg_glucose_7d": avg_glucose_7d,
    "time_since_last_insulin": time_since_last_insulin,
    "hyper_events_past_week": hyper_events_past_week,
    "risk_probability": prob,
    "severity_class": severity
})


# Oversample higher severity cases (2, 3, 4) for balance

high_sev_cases = df[df["severity_class"] >= 2]
extra_high_sev = high_sev_cases.sample(frac=0.2, replace=True, random_state=42)

df = pd.concat([df, extra_high_sev], ignore_index=True)
df = df.sample(frac=1, random_state=42).reset_index(drop=True)


# Save dataset

df.to_csv("simulated_diabetes_data.csv", index=False)

# Summary
print(df.head())
print(f"Final dataset size: {df.shape}")
print(df["severity_class"].value_counts(normalize=True))

# logistic_regression

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix # Import confusion_matrix
from sklearn.exceptions import ConvergenceWarning
import joblib
import warnings
from termcolor import colored
from colorama import init
from imblearn.over_sampling import RandomOverSampler, SMOTE # Import SMOTE
from collections import Counter # Import Counter
import numpy as np # Import numpy

# Suppress convergence warnings
warnings.filterwarnings("ignore", category=ConvergenceWarning)
init(autoreset=True)

# Initialize results dictionary if it doesn't exist
if 'results' not in globals():
    results = {}


# STEP 1: Load Dataset

df = pd.read_csv("simulated_diabetes_data.csv")

feature_cols = [
    "glucose", "carbs", "insulin", "activity", "stress", "time_of_day",
    "age", "bmi", "hba1c", "hypertension", "kidney_disease", "type_diabetes",
    "duration_since_dx", "sleep_hours", "diet_quality", "smoking_status",
    "alcohol_units_week", "med_adherence", "glucose_trend_3h",
    "avg_glucose_7d", "time_since_last_insulin", "hyper_events_past_week"
]

X = df[feature_cols]
y = df["severity_class"]

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)


# STEP 2: Feature Scaling

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# STEP 3: Handle Class Imbalance

train_counts = Counter(y_train)
print(colored(f"Class distribution before balancing: {dict(train_counts)}", "cyan"))

min_samples = min(train_counts.values())

if min_samples > 1:
    try:
        k_neighbors_smote = min(min_samples - 1, 5) if min_samples > 1 else 1
        if k_neighbors_smote < 1:
             raise ValueError("Cannot apply SMOTE with less than 2 samples in the smallest class.")
        smote = SMOTE(random_state=42, k_neighbors=k_neighbors_smote)
        X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)
        print(colored(" SMOTE applied successfully", "green"))
    except ValueError as e:
        print(colored(f" SMOTE failed: {e}. Using RandomOverSampler instead.", "red"))
        ros = RandomOverSampler(random_state=42)
        X_train_res, y_train_res = ros.fit_resample(X_train_scaled, y_train)
        print(colored(" RandomOverSampler applied", "green"))
else:
    print(colored(" Not enough samples in one class for SMOTE. Using RandomOverSampler.", "red"))
    ros = RandomOverSampler(random_state=42)
    X_train_res, y_train_res = ros.fit_resample(X_train_scaled, y_train)
    print(colored(" RandomOverSampler applied", "green"))

print(colored(f" Class distribution after balancing: {dict(Counter(y_train_res))}", "cyan"))



# STEP 4: Logistic Regression Model

logreg = LogisticRegression(
    multi_class="multinomial",
    solver="saga",
    max_iter=2000,
    C=0.5,
    random_state=42
)

print(colored("Training Logistic Regression with scaling & saga solver...", "yellow", attrs=["bold"]))
logreg.fit(X_train_res, y_train_res) # Use resampled data for training


# STEP 5: Evaluation

y_pred = logreg.predict(X_test_scaled)
acc = accuracy_score(y_test, y_pred)
print(colored(f"Logistic Regression Accuracy: {acc:.4f}", "green", attrs=["bold"]))
print(classification_report(y_test, y_pred, zero_division=0))


# Add Logistic Regression results to the results dictionary
results["LogisticRegression"] = {
    "report": classification_report(y_test, y_pred, output_dict=True, zero_division=0),
    "confusion_matrix": confusion_matrix(y_test, y_pred),
    "pred_proba": logreg.predict_proba(X_test_scaled)
}


# STEP 6: Save Model & Coefficients

joblib.dump((logreg, scaler), "LogisticRegression_Multinomial_model.pkl")
print(colored("Model & scaler saved as LogisticRegression_Multinomial_model.pkl", "magenta", attrs=["bold"]))

# Coefficients table
coefs = logreg.coef_
classes = logreg.classes_
coef_df = pd.DataFrame(coefs.T, index=feature_cols, columns=[f"Class_{cls}" for cls in classes])

# Save coefficients with color gradient
coef_styled = coef_df.style.background_gradient(cmap="coolwarm").set_caption("Logistic Regression Coefficients")
try:
    coef_styled.to_excel("logistic_regression_coefficients_colored.xlsx", engine="openpyxl")
    print(colored("Saved logistic_regression_coefficients_colored.xlsx", "cyan"))
except ImportError:
    print(colored("Could not save coefficients to Excel. Please install openpyxl (`pip install openpyxl`).", "red"))
    coef_df.to_csv("simulated_diabetes_data.csv")
    print(colored(""saved simulated_diabetes_data.csv" instead.", "cyan"))

import streamlit as st
import pandas as pd
import numpy as np
import joblib
import plotly.express as px

# -------------------------
# Load saved model and scaler
# -------------------------
model = joblib.load("LogisticRegression_Multinomial_model.pkl")
scaler = joblib.load("scaler.pkl")

features = [
    "age", "bmi", "hba1c", "avg_glucose_7d", "carbs", "insulin",
    "activity", "sleep_hours", "stress_level", "diet_quality",
    "smoking", "alcohol", "med_adherence", "hypertension", "kidney_disease"
]

class_labels = {
    0: "Normal",
    1: "Mild Hyperglycemia",
    2: "Moderate Hyperglycemia",
    3: "Severe Hyperglycemia"
}

suggestions = {
    "Normal": "Maintain current lifestyle, continue regular monitoring.",
    "Mild Hyperglycemia": "Review diet and activity levels, consider adjusting insulin/carbs.",
    "Moderate Hyperglycemia": "Increase physical activity, improve diet, consult physician soon.",
    "Severe Hyperglycemia": "Seek urgent medical advice. Medication or insulin adjustment may be required."
}

# -------------------------
# Streamlit UI
# -------------------------
st.title("🩺 Hyperglycemia Risk Calculator (Logistic Regression)")

st.markdown("Enter patient details to predict hyperglycemia risk:")

# Collect inputs
inputs = {}
inputs["age"] = st.slider("Age", 18, 90, 45)
inputs["bmi"] = st.slider("BMI", 15.0, 45.0, 25.0)
inputs["hba1c"] = st.slider("HbA1c (%)", 4.5, 14.0, 6.0)
inputs["avg_glucose_7d"] = st.slider("Average Glucose (7 days)", 60, 400, 120)
inputs["carbs"] = st.slider("Carbohydrate Intake (g/day)", 50, 500, 250)
inputs["insulin"] = st.slider("Insulin Dose (units/day)", 0, 100, 20)
inputs["activity"] = st.slider("Physical Activity (mins/day)", 0, 180, 30)
inputs["sleep_hours"] = st.slider("Sleep Hours", 3, 12, 7)
inputs["stress_level"] = st.slider("Stress Level (1=low, 10=high)", 1, 10, 5)
inputs["diet_quality"] = st.slider("Diet Quality (1=poor, 10=excellent)", 1, 10, 5)
inputs["smoking"] = st.selectbox("Smoking", [0, 1])
inputs["alcohol"] = st.selectbox("Alcohol Consumption", [0, 1])
inputs["med_adherence"] = st.slider("Medication Adherence (%)", 0, 100, 80)
inputs["hypertension"] = st.selectbox("Hypertension", [0, 1])
inputs["kidney_disease"] = st.selectbox("Kidney Disease", [0, 1])

input_df = pd.DataFrame([inputs])

# -------------------------
# Prediction
# -------------------------
if st.button("Predict Risk"):
    input_scaled = scaler.transform(input_df)
    probs = model.predict_proba(input_scaled)[0]
    pred_class = np.argmax(probs)
    label = class_labels[pred_class]

    st.subheader(f"Predicted Risk: **{label}**")
    st.markdown(f"💡 Suggestion: {suggestions[label]}")

    # Probability bar chart
    prob_df = pd.DataFrame({
        "Risk Category": [class_labels[i] for i in range(len(probs))],
        "Probability": probs
    })

    fig = px.bar(prob_df, x="Risk Category", y="Probability",
                 color="Risk Category", text="Probability",
                 color_discrete_sequence=px.colors.qualitative.Set2)

    fig.update_traces(texttemplate='%{text:.2f}', textposition="outside")
    fig.update_layout(yaxis=dict(range=[0,1]), showlegend=False)

    st.plotly_chart(fig, use_container_width=True)
