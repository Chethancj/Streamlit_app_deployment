# -*- coding: utf-8 -*-
"""streamlit code 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M-3GVh8sxDfpebbzP39YrZCGKHMfxOY2
"""

import numpy as np
import pandas as pd

# For reproducibility
np.random.seed(42)

# Number of base samples
n_samples = 40000


# physiological features

glucose = np.random.normal(loc=140, scale=30, size=n_samples)  # mg/dL
carbs = np.random.normal(loc=50, scale=15, size=n_samples)  # grams
insulin = np.random.normal(loc=8, scale=3, size=n_samples)  # units
activity = np.random.normal(loc=30, scale=10, size=n_samples)  # minutes/day
stress = np.random.choice([0, 1], size=n_samples, p=[0.7, 0.3])  # 0 = low, 1 = high
time_of_day = np.random.choice([0, 1, 2], size=n_samples, p=[0.3, 0.4, 0.3])  # 0=morning, 1=afternoon, 2=evening


#  clinical features

age = np.random.normal(loc=55, scale=12, size=n_samples).clip(18, 90)
bmi = np.random.normal(loc=28, scale=5, size=n_samples).clip(15, 50)
hba1c = np.random.normal(loc=7.5, scale=1.2, size=n_samples).clip(4, 15)
hypertension = np.random.choice([0, 1], size=n_samples, p=[0.65, 0.35])
kidney_disease = np.random.choice([0, 1], size=n_samples, p=[0.85, 0.15])
type_diabetes = np.random.choice([1, 2], size=n_samples, p=[0.3, 0.7])
duration_since_dx = np.random.normal(loc=8, scale=6, size=n_samples).clip(0, 50)  # years


# Lifestyle features

sleep_hours = np.random.normal(loc=7, scale=1.5, size=n_samples).clip(3, 12)
diet_quality = np.random.choice([1, 2, 3], size=n_samples, p=[0.3, 0.5, 0.2])  # 1=poor, 3=good
smoking_status = np.random.choice([0, 1, 2], size=n_samples, p=[0.6, 0.25, 0.15])  # 0=non,1=former,2=current
alcohol_units_week = np.random.poisson(lam=3, size=n_samples)  # units/week
med_adherence = np.random.choice([0, 1, 2], size=n_samples, p=[0.7, 0.2, 0.1])  # 0=adherent,1=missed,2=irregular


# Time-trend glucose features

glucose_trend_3h = np.random.choice([-1, 0, 1], size=n_samples, p=[0.25, 0.5, 0.25])  # -1=falling,0=stable,1=rising
avg_glucose_7d = np.random.normal(loc=145, scale=25, size=n_samples)
time_since_last_insulin = np.random.normal(loc=4, scale=2, size=n_samples).clip(0, 24)  # hours
hyper_events_past_week = np.random.poisson(lam=2, size=n_samples)


#  probabilities for multi-class hyperglycaemia

# We'll create a continuous risk score and then map to severity categories
beta = {
    "intercept": -5,
    "glucose": 0.035,
    "carbs": 0.015,
    "insulin": -0.04,
    "activity": -0.02,
    "stress": 0.7,
    "time_afternoon": 0.25,
    "time_evening": 0.45,
    "age": 0.008,
    "bmi": 0.015,
    "hba1c": 0.35,
    "hypertension": 0.4,
    "kidney_disease": 0.6,
    "type_diabetes": 0.3,
    "duration_since_dx": 0.01,
    "sleep_hours": -0.05,
    "diet_quality": -0.1,
    "smoking_status": 0.15,
    "alcohol_units_week": 0.02,
    "med_adherence": 0.4,
    "glucose_trend_3h": 0.5,
    "avg_glucose_7d": 0.02,
    "time_since_last_insulin": 0.03,
    "hyper_events_past_week": 0.25
}

time_afternoon = (time_of_day == 1).astype(int)
time_evening = (time_of_day == 2).astype(int)

logit = (
    beta["intercept"]
    + beta["glucose"] * glucose
    + beta["carbs"] * carbs
    + beta["insulin"] * insulin
    + beta["activity"] * activity
    + beta["stress"] * stress
    + beta["time_afternoon"] * time_afternoon
    + beta["time_evening"] * time_evening
    + beta["age"] * age
    + beta["bmi"] * bmi
    + beta["hba1c"] * hba1c
    + beta["hypertension"] * hypertension
    + beta["kidney_disease"] * kidney_disease
    + beta["type_diabetes"] * (type_diabetes - 1)  # type 2 gets 0, type 1 gets coeff
    + beta["duration_since_dx"] * duration_since_dx
    + beta["sleep_hours"] * sleep_hours
    + beta["diet_quality"] * diet_quality
    + beta["smoking_status"] * smoking_status
    + beta["alcohol_units_week"] * alcohol_units_week
    + beta["med_adherence"] * med_adherence
    + beta["glucose_trend_3h"] * glucose_trend_3h
    + beta["avg_glucose_7d"] * avg_glucose_7d
    + beta["time_since_last_insulin"] * time_since_last_insulin
    + beta["hyper_events_past_week"] * hyper_events_past_week
)

prob = 1 / (1 + np.exp(-logit))


# severity categories
severity_labels = ['Hypo', 'Normal', 'Mild Hyperglycemia', 'Moderate Hyperglycemia', 'Severe Hyperglycemia']
severity_classes = [0, 1, 2, 3]
severity = np.select(
    [
        glucose < 70,
        (glucose >= 70) & (glucose < 180),
        (glucose >= 180) & (glucose < 250),
        (glucose >= 250) & (glucose < 300),
        glucose >= 300
    ],
    [0, 1, 2, 3, 4]
)


# Build DataFrame

df = pd.DataFrame({
    "glucose": glucose,
    "carbs": carbs,
    "insulin": insulin,
    "activity": activity,
    "stress": stress,
    "time_of_day": time_of_day,
    "age": age,
    "bmi": bmi,
    "hba1c": hba1c,
    "hypertension": hypertension,
    "kidney_disease": kidney_disease,
    "type_diabetes": type_diabetes,
    "duration_since_dx": duration_since_dx,
    "sleep_hours": sleep_hours,
    "diet_quality": diet_quality,
    "smoking_status": smoking_status,
    "alcohol_units_week": alcohol_units_week,
    "med_adherence": med_adherence,
    "glucose_trend_3h": glucose_trend_3h,
    "avg_glucose_7d": avg_glucose_7d,
    "time_since_last_insulin": time_since_last_insulin,
    "hyper_events_past_week": hyper_events_past_week,
    "risk_probability": prob,
    "severity_class": severity
})


# Oversample higher severity cases (2, 3, 4) for balance

high_sev_cases = df[df["severity_class"] >= 2]
extra_high_sev = high_sev_cases.sample(frac=0.2, replace=True, random_state=42)

df = pd.concat([df, extra_high_sev], ignore_index=True)
df = df.sample(frac=1, random_state=42).reset_index(drop=True)


# Save dataset

df.to_csv("simulated_diabetes_data.csv", index=False)

# Summary
print(df.head())
print(f"Final dataset size: {df.shape}")
print(df["severity_class"].value_counts(normalize=True))

# logistic_regression

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix # Import confusion_matrix
from sklearn.exceptions import ConvergenceWarning
import joblib
import warnings
from termcolor import colored
from colorama import init
from imblearn.over_sampling import RandomOverSampler, SMOTE # Import SMOTE
from collections import Counter # Import Counter
import numpy as np # Import numpy

# Suppress convergence warnings
warnings.filterwarnings("ignore", category=ConvergenceWarning)
init(autoreset=True)

# Initialize results dictionary if it doesn't exist
if 'results' not in globals():
    results = {}


# STEP 1: Load Dataset

df = pd.read_csv("simulated_diabetes_data.csv")

feature_cols = [
    "glucose", "carbs", "insulin", "activity", "stress", "time_of_day",
    "age", "bmi", "hba1c", "hypertension", "kidney_disease", "type_diabetes",
    "duration_since_dx", "sleep_hours", "diet_quality", "smoking_status",
    "alcohol_units_week", "med_adherence", "glucose_trend_3h",
    "avg_glucose_7d", "time_since_last_insulin", "hyper_events_past_week"
]

X = df[feature_cols]
y = df["severity_class"]

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)


# STEP 2: Feature Scaling

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform((X_test))


# STEP 3: Handle Class Imbalance

train_counts = Counter(y_train)
print(colored(f"Class distribution before balancing: {dict(train_counts)}", "cyan"))

min_samples = min(train_counts.values())

if min_samples > 1:
    try:
        k_neighbors_smote = min(min_samples - 1, 5) if min_samples > 1 else 1
        if k_neighbors_smote < 1:
             raise ValueError("Cannot apply SMOTE with less than 2 samples in the smallest class.")
        smote = SMOTE(random_state=42, k_neighbors=k_neighbors_smote)
        X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)
        print(colored(" SMOTE applied successfully", "green"))
    except ValueError as e:
        print(colored(f" SMOTE failed: {e}. Using RandomOverSampler instead.", "red"))
        ros = RandomOverSampler(random_state=42)
        X_train_res, y_train_res = ros.fit_resample(X_train_scaled, y_train)
        print(colored(" RandomOverSampler applied", "green"))
else:
    print(colored(" Not enough samples in one class for SMOTE. Using RandomOverSampler.", "red"))
    ros = RandomOverSampler(random_state=42)
    X_train_res, y_train_res = ros.fit_resample(X_train_scaled, y_train)
    print(colored(" RandomOverSampler applied", "green"))

print(colored(f" Class distribution after balancing: {dict(Counter(y_train_res))}", "cyan"))



# STEP 4: Logistic Regression Model

logreg = LogisticRegression(
    multi_class="multinomial",
    solver="saga",
    max_iter=2000,
    C=0.5,
    random_state=42
)

print(colored("Training Logistic Regression with scaling & saga solver...", "yellow", attrs=["bold"]))
logreg.fit(X_train_res, y_train_res) # Use resampled data for training


# STEP 5: Evaluation

y_pred = logreg.predict(X_test_scaled)
acc = accuracy_score(y_test, y_pred)
print(colored(f"Logistic Regression Accuracy: {acc:.4f}", "green", attrs=["bold"]))
print(classification_report(y_test, y_pred, zero_division=0))


# Add Logistic Regression results to the results dictionary
results["LogisticRegression"] = {
    "report": classification_report(y_test, y_pred, output_dict=True, zero_division=0),
    "confusion_matrix": confusion_matrix(y_test, y_pred),
    "pred_proba": logreg.predict_proba(X_test_scaled)
}


# STEP 6: Save Model & Coefficients

joblib.dump((logreg, scaler), "LogisticRegression_Multinomial_model.pkl")
print(colored("Model & scaler saved as LogisticRegression_Multinomial_model.pkl", "magenta", attrs=["bold"]))

# Coefficients table
coefs = logreg.coef_
classes = logreg.classes_
coef_df = pd.DataFrame(coefs.T, index=feature_cols, columns=[f"Class_{cls}" for cls in classes])

# Save coefficients with color gradient
coef_styled = coef_df.style.background_gradient(cmap="coolwarm").set_caption("Logistic Regression Coefficients")
try:
    coef_styled.to_excel("logistic_regression_coefficients_colored.xlsx", engine="openpyxl")
    print(colored("Saved logistic_regression_coefficients_colored.xlsx", "cyan"))
except ImportError:
    print(colored("Could not save coefficients to Excel. Please install openpyxl (`pip install openpyxl`).", "red"))
    coef_df.to_csv("simulated_diabetes_data.csv")
    

# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import joblib
import plotly.express as px

# -------------------------
# Load saved model and scaler
# -------------------------
def load_model():
    try:
        loaded_object = joblib.load("LogisticRegression_Multinomial_model.pkl")
        if not isinstance(loaded_object, tuple) or len(loaded_object) != 2:
            # This check might be redundant if joblib.load raises an error for incompatible files
            # but keeping it for clarity.
            raise TypeError("Error loading model: Expected a tuple of (model, scaler).")

        model, scaler = loaded_object

        # Feature columns should ideally be saved with the model or scaler
        # For now, we use the predefined list from the training script.
        feature_cols = [
            "glucose", "carbs", "insulin", "activity", "stress", "time_of_day",
            "age", "bmi", "hba1c", "hypertension", "kidney_disease", "type_diabetes",
            "duration_since_dx", "sleep_hours", "diet_quality", "smoking_status",
            "alcohol_units_week", "med_adherence", "glucose_trend_3h",
            "avg_glucose_7d", "time_since_last_insulin", "hyper_events_past_week"
        ]
        return model, scaler, feature_cols
    except FileNotFoundError:
        st.error("Error loading model: LogisticRegression_Multinomial_model.pkl not found.")
        raise # Re-raise the exception after logging
    except Exception as e:
        st.error(f"Failed to load model: {str(e)}")
        raise # Re-raise the exception to see the traceback

model, scaler, feature_cols = load_model()

# Define class labels and suggestions
class_labels = {
    0: "Hypo",
    1: "Normal",
    2: "Mild Hyperglycemia",
    3: "Moderate Hyperglycemia",
    4: "Severe Hyperglycemia"
}

suggestions = {
    "Hypo": "Seek urgent medical advice.",
    "Normal": "Maintain current lifestyle, continue regular monitoring.",
    "Mild Hyperglycemia": "Review diet and activity levels, consider adjusting insulin/carbs.",
    "Moderate Hyperglycemia": "Increase physical activity, improve diet, consult physician soon.",
    "Severe Hyperglycemia": "Seek urgent medical advice. Medication or insulin adjustment may be required."
}

# -------------------------
# Streamlit UI
# -------------------------
st.title("ðŸ©º Hyperglycemia Risk Calculator (Logistic Regression)")
st.markdown("Enter patient details to predict hyperglycemia risk:")

# Input collection with validation
def get_user_inputs():
    inputs = {}
    col1, col2 = st.columns(2)

    with col1:
        inputs["glucose"] = st.slider("Glucose (mg/dL)", 40, 400, 140)
        inputs["carbs"] = st.slider("Carbohydrate Intake (grams)", 0, 100, 50)
        inputs["insulin"] = st.slider("Insulin (units)", 0, 20, 8)
        inputs["activity"] = st.slider("Physical Activity (minutes/day)", 0, 180, 30)
        inputs["stress"] = st.selectbox("Stress (0=low, 1=high)", [0, 1])
        inputs["time_of_day"] = st.selectbox("Time of Day (0=morning, 1=afternoon, 2=evening)", [0, 1, 2])
        inputs["age"] = st.slider("Age", 18, 90, 55)
        inputs["bmi"] = st.slider("BMI", 15.0, 50.0, 28.0)
        inputs["hba1c"] = st.slider("HbA1c (%)", 4.0, 15.0, 7.5)

    with col2:
        inputs["hypertension"] = st.selectbox("Hypertension (0=no, 1=yes)", [0, 1])
        inputs["kidney_disease"] = st.selectbox("Kidney Disease (0=no, 1=yes)", [0, 1])
        inputs["type_diabetes"] = st.selectbox("Type of Diabetes (1=Type 1, 2=Type 2)", [1, 2])
        inputs["duration_since_dx"] = st.slider("Duration since diagnosis (years)", 0, 50, 8)
        inputs["sleep_hours"] = st.slider("Sleep Hours", 3, 12, 7)
        inputs["diet_quality"] = st.selectbox("Diet Quality (1=poor, 2=average, 3=good)", [1, 2, 3])
        inputs["smoking_status"] = st.selectbox("Smoking Status (0=non, 1=former, 2=current)", [0, 1, 2])
        inputs["alcohol_units_week"] = st.slider("Alcohol Units per Week", 0, 20, 3)
        inputs["med_adherence"] = st.selectbox("Medication Adherence (0=adherent, 1=missed, 2=irregular)", [0, 1, 2])

    inputs["glucose_trend_3h"] = st.selectbox("Glucose Trend (3h) (-1=falling, 0=stable, 1=rising)", [-1, 0, 1])
    inputs["avg_glucose_7d"] = st.slider("Average Glucose (7 days) (mg/dL)", 60, 400, 145)
    inputs["time_since_last_insulin"] = st.slider("Time since last insulin (hours)", 0, 24, 4)
    inputs["hyper_events_past_week"] = st.slider("Hyperglycemic events past week", 0, 10, 2)

    return inputs

# -------------------------
# Prediction Function
# -------------------------
def make_prediction(input_data):
    try:
        # Create DataFrame and ensure correct column order
        input_df = pd.DataFrame([input_data])
        input_df = input_df[feature_cols]  # Ensure correct column order

        # Scale the input data
        input_scaled = scaler.transform(input_df)

        # Make prediction
        probs = model.predict_proba(input_scaled)[0]
        pred_class = np.argmax(probs)

        return probs, pred_class

    except Exception as e:
        st.error(f"Prediction failed: {str(e)}")
        st.write("Debug Info:")
        st.write("Input DataFrame columns:", input_df.columns.tolist())
        st.write("Expected columns:", feature_cols)
        return None, None

# -------------------------
# Main App
# -------------------------
inputs = get_user_inputs()

if st.button("Predict Risk"):
    if model is None or scaler is None:
        st.error("Model or scaler not loaded. Please check the logs for details.")
    else:
        with st.spinner("Calculating risk..."):
            probs, pred_class = make_prediction(inputs)

            if pred_class is not None:
                label = class_labels[pred_class]

                # Display results
                st.subheader(f"Predicted Risk: **{label}**")
                st.markdown(f"ðŸ’¡ Suggestion: {suggestions[label]}")

                # Probability visualization
                prob_df = pd.DataFrame({
                    "Risk Category": [class_labels[i] for i in range(len(probs))],
                    "Probability": probs
                })

                fig = px.bar(prob_df, x="Risk Category", y="Probability",
                            color="Risk Category", text="Probability",
                            color_discrete_sequence=px.colors.qualitative.Set2)

                fig.update_traces(texttemplate='%{text:.2f}', textposition="outside")
                fig.update_layout(yaxis=dict(range=[0,1]), showlegend=False,
                                title="Predicted Probability Distribution")

                st.plotly_chart(fig, use_container_width=True)
